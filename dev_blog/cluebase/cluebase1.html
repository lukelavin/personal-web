<!DOCTYPE html>
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../blog.css">
  <link rel="shortcut icon" type="image/x-icon" href="../..favicon.ico">
  <script src="https://unpkg.com/zdog@1/dist/zdog.dist.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/zfont/dist/zfont.min.js"></script>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/highlight.min.js"></script>
  <link rel="stylesheet" href="../gruvbox-highlighting.css">
  <script>hljs.highlightAll();</script>
  <title>Luke Lavin - Home</title>

  <!-- Primary Meta Tags -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="title" content="Luke Lavin - Home">
  <meta name="description"
    content="Hi! I'm Luke Lavin, and this is my personal website. You can find my projects and contact info here.">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lukelav.in/">
  <meta property="og:title" content="Luke Lavin - Home">
  <meta property="og:description"
    content="Hi! I'm Luke Lavin, and this is my personal website. You can find my projects and contact info here.">
  <meta property="og:image" content="https://lukelav.in/card.png">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://lukelav.in/">
  <meta property="twitter:title" content="Luke Lavin - Home">
  <meta property="twitter:description"
    content="Hi! I'm Luke Lavin, and this is my personal website. You can find my projects and contact info here.">
  <meta property="twitter:image" content="https://lukelav.in/card.png">

</head>

<body>
  <div class="layout-container">

    <div class="sticky">

      <div class="sidebar">
        <div class="home-icon">
          <svg class="zdog-canvas" width="200" height="120"></svg>
          <script src="../blog.js"></script>
          <a class="home-icon-link" href="../../index.html">
          </a>
        </div>

        <div class="sidebar-nav">
          <div class="sidebar-list">
            <div class="sidebar-li"><a class="sidebar-link" href=../index.html>Blog Home</a></div>
            <div class="article-list">
              <a class="article-list-link" href="cluebase0.html">
                <div class="article-list-text">Cluebase - A Case Study</div>
              </a>
              <a class="article-list-link" href="cluebase1.html">
                <div class="article-list-text"> - Part 1: Scraping</div>
              </a>
              <a class="article-list-link" href="cluebase2.html">
                <div class="article-list-text"> - Part 2: Data Load</div>
              </a>
              <a class="article-list-link" href="cluebase3.html">
                <div class="article-list-text"> - Part 3: Automation</div>
              </a>
              <a class="article-list-link" href="cluebase4.html">
                <div class="article-list-text"> - Part 4: Basic API</div>
              </a>
              <a class="article-list-link" href="cluebase5.html">
                <div class="article-list-text"> - Part 5: Basic Front End</div>
              </a>
              <a class="article-list-link" href="cluebase6.html">
                <div class="article-list-text"> - Part 6: A Better Category
                </div>
              </a>
            </div>
            <div class="nextPrev">
              <a class="sidebar-link" href=cluebase0.html>
                <div class="sidebar-li">
                  < Prev </div>
              </a>
              <a class="sidebar-link" href=cluebase2.html>
                <div class="sidebar-li">
                  Next >
                </div>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="blog-container">
      <div class="blog-title">Cluebase 2.0 - Part 1: Scraping</div>
      <div class="blog-subtitle">by Luke Lavin, published November 10th, 2024</div>
      <hr>
      <div markdown="1" class="blog-content">

        <p>
          As discussed in the <a href="cluebase0.html">Cluebase introduction</a>, the original Cluebase data
          was gathered from a one-time process run on my local machine. At the time, I just wanted to get
          decent data quickly, so I could focus on developing the API, so there was little consideration for
          data quality. Additionally, putting effort into the reusability and stability of this one-time
          process would have been a waste.
        </p>

        <p>In other words, there's a lot of work to do in the scraping process. Not much of the existing scraper is
          usable, so it seems best to just start fresh with a new focus on doing it the right way.</p>

        <h2>Scraping Goals</h2>

        <h3>1. Data Freshness</h3>

        <p>
          Probably the biggest oversight in the original process was that it was meant to be run only once. After
          running the scraping
          process a single time, there was no easy way to run it again to update the data. In order to include newly
          aired clues, I
          would need to redownload all html files temporarily, parse the clues from each, load them into the database
          skipping any duplicates,
          and then generate a new data dump to use within the database container.
        </p>

        <p>
          The new web scraping process should be able to grab the latest data incrementally, which means it needs to
          track what has been downloaded already, and notice any possible updates.
        </p>
        <p>
          Additionally, the process should be
          able to run in an automated setting to keep the database up to date without my manual intervention--no more
          image builds just to insert into the database.
        </p>

        <h3>2. Coupled Scrape/Load process</h3>
        <p>Previously, the scraping process included parsing the clues from the HTML and loading them into the db. This
          meant that in order to change any
          parsing/loading/formatting logic I would need to redownload every html page, which would take
          hours using a respectful rate limit for the fragile J! Archive servers. </p>

        <p>Scraping links and downloading the HTML should be separate from parsing and loading the clues.
          By keeping a data lake of raw HTML, I can iterate on the parse/load process as many times as necessary,
          while only having to download each HTML page once, saving hours of waiting on rate limited requests.
          Doing so will also greatly aid in solving the challenges in the actual load process.
        </p>

        <h2 id="plan-">Planning the Implementation</h2>
        <p>So, we want a way to parse the crawl the webpages of J! Archive, downloading new game/clue files as we move
          along. This problem can be broken down simply into two problems: grabbing links while traversing the archive
          and determining which links are to HTML pages that need to be downloaded.</p>

        <h3>Crawling</h3>

        <p>Thankfully, the first part is more or less solved, web crawlers are about as old as the web itself.
          Generally, a web crawler loads an HTML page, parses the HTML to grab all link tags, and queue them each to
          visit next. An extra-intelligent crawler would maybe use a smart queue that prioritizes highly ranked pages
          (by traditional PageRank, visits, traffic, update frequency, or other factors). </p>

        <p>For us, though, crawling will be even simpler, because we have the benefit of J! Archive's unchanging
          structure. We only need the HTML data from 3 urls.</p>

        <pre><code>/listseasons.php
/showseason.php?season=&lt;id&gt;
/showgame.php?game_id=&lt;id&gt;
</code></pre>

        <p>The <code>listseasons</code> page holds a table of all seasons, with each season having a reference to its
          <code>showseason</code> page. Here's what it looks like to a human user:
        </p>
        <img src="img/listseasons.png" class="fullwidth-image"><img>

        <p>Similarly, the <code>showseason</code> page holds a table of all games in the season, with each game having a
          reference to its <code>showgame</code> page.
        </p>
        <img src="img/showseason.png" class="fullwidth-image"><img>

        <p>And once we're at the <code>showgame</code> level, the clue texts and solutions are present directly in the
          HTML. We can stop here; downloading this HTML will give the next parse/load process all the info it needs.</p>
        <img src="img/showgame.png" class="fullwidth-image"><img>

        <h3>Downloading</h3>

        <p>We'll already have the HTML loaded for each of these pages, so to complete the "download" process, we just
          need to write them to disk. The tricky
          part here will be limiting the downloads to just files we need, so what exactly consitutes a file that we
          need?</p>

        <p>
          We want to include all clues, so we need all game files. Any game file that is not already downloaded should
          be downloaded, and any game file that has been updated should be downloaded. For simplicity's sake we can just
          start with downloading all <code>showgames</code> pages, indexing them by their game ids. If during crawling
          we come across a showgames page for a game id we don't have a file for, we make sure to download it before
          continuing the crawling process.
        </p>

        <p>It probably also makes sense to keep records of the <code>listseasons</code> page and each
          <code>showseasons</code> page, since we pick them up along the way anyways.
        </p>

        <h3 id="dev-progress-">Dev progress:</h3>
        <p>Currently: Focus on keeping good raw HTML data.</p>
        <ul>
          <li>Downloading season list HTML</li>
          <li>Scraping season list HTML for season links, then downloading those HTML files</li>
          <li>Scraping season links for game links, then downloading those HTML files</li>
        </ul>
        <p>Current problems I&#39;m running into are related to the process being long and flaky. While the
          current
          flakiness comes from me starting and stopping the process frequently due to active development,
          network
          and
          file actions are inherently flaky, so a long running process dependent on the stability of these is
          asking
          for problems. </p>
        <p>The constant start and stop of the process raises the following concerns:</p>
        <ol>
          <li>Data integrity: How do we know a file was fully written, or properly written. How do we know a
            request
            completed successfully, and returned the full HTML we want.</li>
          <li>Processing status: How do we know when we&#39;ve already processed an HTML link. Can we skip
            files
            we
            already have to try to just pick back where we left off? If we do, how do we know when a file is
            out
            of
            date, and actually needs to be updated?</li>
        </ol>
        <p>TODO: add some validation checks on html files. Most of the </p>
        <p>Only intelligence is that it will not overwrite a html file that already exists</p>
        <ul>
          <li>is this how it should work? what if a page exists but is only partially complete (this happens
            every
            night!)</li>
          <li>maybe won&#39;t matter if there&#39;s a scheduled &quot;refresh&quot; that ignores overwrite
            protection
          </li>
        </ul>

      </div>
    </div>
  </div>
</body>