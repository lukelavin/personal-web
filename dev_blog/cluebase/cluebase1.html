<!DOCTYPE html>
<html>

<head>
  <link rel="stylesheet" type="text/css" href="../blog.css">
  <link rel="shortcut icon" type="image/x-icon" href="../..favicon.ico">
  <script src="https://unpkg.com/zdog@1/dist/zdog.dist.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/zfont/dist/zfont.min.js"></script>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/highlight.min.js"></script>
  <link rel="stylesheet" href="../gruvbox-highlighting.css">
  <script>hljs.highlightAll();</script>
  <title>Luke Lavin - Home</title>

  <!-- Primary Meta Tags -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="title" content="Luke Lavin - Home">
  <meta name="description"
    content="Hi! I'm Luke Lavin, and this is my personal website. You can find my projects and contact info here.">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lukelav.in/">
  <meta property="og:title" content="Luke Lavin - Home">
  <meta property="og:description"
    content="Hi! I'm Luke Lavin, and this is my personal website. You can find my projects and contact info here.">
  <meta property="og:image" content="https://lukelav.in/card.png">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://lukelav.in/">
  <meta property="twitter:title" content="Luke Lavin - Home">
  <meta property="twitter:description"
    content="Hi! I'm Luke Lavin, and this is my personal website. You can find my projects and contact info here.">
  <meta property="twitter:image" content="https://lukelav.in/card.png">

</head>

<body>
  <div class="layout-container">

    <div class="sticky">

      <div class="sidebar">
        <div class="home-icon">
          <svg class="zdog-canvas" width="200" height="120"></svg>
          <script src="../blog.js"></script>
          <a class="home-icon-link" href="../../index.html">
          </a>
        </div>

        <div class="sidebar-nav">
          <div class="sidebar-list">
            <div class="sidebar-li"><a class="sidebar-link" href=../index.html>Blog Home</a></div>
            <div class="article-list">
              <a class="article-list-link" href="cluebase0.html">
                <div class="article-list-text">Cluebase - A Case Study</div>
              </a>
              <a class="article-list-link" href="cluebase1.html">
                <div class="article-list-text"> - Part 1: Scraping</div>
              </a>
              <a class="article-list-link" href="cluebase2.html">
                <div class="article-list-text"> - Part 2: Data Load</div>
              </a>
              <a class="article-list-link" href="cluebase3.html">
                <div class="article-list-text"> - Part 3: Automation</div>
              </a>
              <a class="article-list-link" href="cluebase4.html">
                <div class="article-list-text"> - Part 4: Basic API</div>
              </a>
              <a class="article-list-link" href="cluebase5.html">
                <div class="article-list-text"> - Part 5: Basic Front End</div>
              </a>
              <a class="article-list-link" href="cluebase6.html">
                <div class="article-list-text"> - Part 6: A Better Category
                </div>
              </a>
            </div>
            <div class="nextPrev">
              <a class="sidebar-link" href=cluebase0.html>
                <div class="sidebar-li">
                  < Prev </div>
              </a>
              <a class="sidebar-link" href=cluebase2.html>
                <div class="sidebar-li">
                  Next >
                </div>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="blog-container">
      <div class="blog-title">Cluebase 2.0 - Part 1: Scraping</div>
      <div class="blog-subtitle">by Luke Lavin, published November 10th, 2024</div>
      <hr>
      <div markdown="1" class="blog-content">

        <p>
          As discussed in the <a href="cluebase0.html">Cluebase introduction</a>, the original Cluebase data
          was gathered from a one-time process run on my local machine. At the time, I just wanted to get
          decent data quickly, so I could focus on developing the API, so there was little consideration for
          data quality. Additionally, putting effort into the reusability and stability of this one-time
          process would have been a waste.
        </p>

        <p>In other words, there's a lot of work to do in the scraping process. Not much of the existing scraper is
          usable, so it seems best to just start fresh with a new focus on doing it the right way.</p>

        <h2>Scraping Goals</h2>

        <h3>1. Data Freshness</h3>

        <p>
          Probably the biggest oversight in the original process was that it was meant to be run only once. After
          running the scraping
          process a single time, there was no easy way to run it again to update the data. In order to include newly
          aired clues, I
          would need to redownload all html files temporarily, parse the clues from each, load them into the database
          skipping any duplicates,
          and then generate a new data dump to use within the database container.
        </p>

        <p>
          The new web scraping process should be able to grab the latest data incrementally, which means it needs to
          track what has been downloaded already, and notice any possible updates.
        </p>
        <p>
          Additionally, the process should be
          able to run in an automated setting to keep the database up to date without my manual intervention--no more
          image builds just to insert into the database.
        </p>

        <h3>2. Coupled Scrape/Load process</h3>
        <p>Previously, the scraping process included parsing the clues from the HTML and loading them into the db. This
          meant that in order to change any
          parsing/loading/formatting logic I would need to redownload every html page, which would take
          hours using a respectful rate limit for the fragile J! Archive servers. </p>

        <p>Scraping links and downloading the HTML should be separate from parsing and loading the clues.
          By keeping a data lake of raw HTML, I can iterate on the parse/load process as many times as necessary,
          while only having to download each HTML page once, saving hours of waiting on rate limited requests.
          Doing so will also greatly aid in solving the challenges in the actual load process.
        </p>

        <h2 id="plan-">Implementation</h2>
        <p>So, we want to crawl the webpages of J! Archive, downloading new game/clue files as we move
          along. Thankfully, web crawlers are about as old as the web itself. </p>

        <p>
          Generally, a web crawler loads an HTML page, parses the HTML to grab all link tags, and queue them each to
          visit next. An extra-intelligent crawler would maybe use a smart queue that prioritizes highly ranked pages
          (by traditional PageRank, visits, traffic, update frequency, or other factors), but we should be able to
          leverage our pre existing knowledge of the archive site to make things simpler for ourselves.
        </p>

        <h3>J! Archive</h3>

        <p>
          Let's take a look at <a href="j-archive.com">J! Archive</a> and see that. Looking at the
          website calls to mind
          a more classic era of the internet, and its simplicity makes it
          relatively easy to see how we can crawl through its games in a simple way.
        </p>

        <pre><code>/listseasons.php
/showseason.php?season=&lt;id&gt;
/showgame.php?game_id=&lt;id&gt;
</code></pre>

        <p>The <code>listseasons</code> page holds a table of all seasons, with each season having a reference to its
          <code>showseason</code> page. There's also some potentially useful info of how many games each season has.
          Here's what it looks like to a human user:
        </p>
        <img src="img/listseasons.png" class="fullwidth-image"><img>

        <p>Drilling down one layer of links into the <code>showseason</code> pages, we can see a similar structure. The
          page holds a table of all games in the season, with each game having a
          reference to its <code>showgame</code> page.
        </p>
        <img src="img/showseason.png" class="fullwidth-image"><img>

        <p>And just one layer down, once we're at the <code>showgame</code> level, the clue texts and solutions are
          present directly in the
          HTML. We can stop here! Downloading this HTML will give the next parse/load process all the info it needs.</p>
        <img src="img/showgame.png" class="fullwidth-image"><img>

        <p>
          With ~40 seasons of a few hundred games each, we get a total of ~10,000 game files. This is a manageable
          enough number that I think the simplest approach is to go down the site map layer-by-layer, downloading the
          list of seasons, then each of the seasons, and then each of the games for each season.
        </p>


        <h3 id="dev-progress-">Dev progress:</h3>
        <p>Currently: Focus on keeping good raw HTML data.</p>
        <ul>
          <li>Downloading season list HTML</li>
          <li>Scraping season list HTML for season links, then downloading those HTML files</li>
          <li>Scraping season links for game links, then downloading those HTML files</li>
        </ul>
        <p>Current problems I&#39;m running into are related to the process being long and flaky. While the
          current
          flakiness comes from me starting and stopping the process frequently due to active development,
          network
          and
          file actions are inherently flaky, so a long running process dependent on the stability of these is
          asking
          for problems. </p>
        <p>The constant start and stop of the process raises the following concerns:</p>
        <ol>
          <li>Data integrity: How do we know a file was fully written, or properly written. How do we know a
            request
            completed successfully, and returned the full HTML we want.</li>
          <li>Processing status: How do we know when we&#39;ve already processed an HTML link. Can we skip
            files
            we
            already have to try to just pick back where we left off? If we do, how do we know when a file is
            out
            of
            date, and actually needs to be updated?</li>
        </ol>
        <p>TODO: add some validation checks on html files. Most of the </p>
        <p>Only intelligence is that it will not overwrite a html file that already exists</p>
        <ul>
          <li>is this how it should work? what if a page exists but is only partially complete (this happens
            every
            night!)</li>
          <li>maybe won&#39;t matter if there&#39;s a scheduled &quot;refresh&quot; that ignores overwrite
            protection
          </li>
        </ul>

      </div>
    </div>
  </div>
</body>